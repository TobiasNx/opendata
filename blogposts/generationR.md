# Replicable business models for replicable science

Replicability requires that the context, the procedures and the data of a scientific experiment be disclosed in sufficient detail for a third party group to repeat that experiment in order to confirm or contest its findings. 

There are a number of threats to replicability. Some of them are technical, some social. Among the technical threats, we can count: 
- Insufficient information about **procedures** (e.g. missing information about temperature) will prevent precise replication. 
- Withholding of **raw data** will prevent statistical analysis by peers. 
- Withholding of **computer code** will not allow to see whether discrepancies in findings might be related to bugs. 
- Absence of **explanatory text** will not prevent replication in the strict sense, but it will make it hard for third parties to understand why a particular setup was chosen in the first place. 

The social threats are more subtle. If an experiment is very nicely motivated, has a good description, and all raw data and computational procedures are available, but only behind a paywall, this will bar some groups from trying to replicate the findings. **The publication ecosystem is thus also a component which influences replicability**. A publication ecosystem which is built on making access to information scarce for financial gain ("reader-pays", the traditional subscription model) is in itself an enemy of replicability.

Open Access, Open Data, and more broadly Open Science have the set goal to overcome these legacy publication systems, which have the restriction of access as their core business model. But which newer setups work, and which ones don't? We are thus back to a new empirical question, this time of an entrepreneurial nature: how to setup a [sustainable publication ecosystem](https://cameronneylon.net/blog/principles-for-open-scholarly-infrastructures/) which furthers replicability? 

In a way, a publishing platform compares quite well with an experiment: you have a context (your subfield), some procedures (workflows, toolchains), and computer code/software. All this together yields (business) data. The logical step is to make these available for replication. This is what has been done in the OpenAire-project *Full disclosure: replicable strategies for book publications supplemented with empirical data*. This project was run by [Language Science Press](http://www.langsci-press.org). LangSci released the following items: 

- their [business model from 2015](https://zenodo.org/record/1286972)
- their [business data from 2017](https://github.com/langsci/opendata/tree/master/business%20data%202017) including expenditures, sales, downloads etc 
- a [spreadsheet](https://github.com/langsci/opendata/tree/master/calculations) to calculate earnings and expenditures based on 100 variables such as cost of labour, length of books, time spent on typesetting a page,  setup costs for print-on-demand, etc. 
- a [cookbook](https://zenodo.org/record/1286925) with best practices, lessons learned, and other insights gained in the course of the project since 2014

The business model from 2015 analyses the publishing landscape and identifies four target groups: 
1. authors, 
1. readers, 
1. libraries, and 
1. research institutions. 

These are matched with five revenue streams: 
1. print copies, 
1. author fees, 
1. institutional memberships, 
1. individual memberships, and 
1. donations. 

The 2015 business model contains some projections about the earnings to expect from each of those sources. The document released in 2018 contains annotations and evaluations of these projections. Basically, of the four streams, only institutional memberships met the expectations; the performance of the other revenue streams was way below par. We thus have an elaborate theoretical model, which made predictions, and we have the business data to evaluate those predictions. We furthermore have the computational tools to adjust the model: the spreadsheet. The context ist given by the "cookbook", which contains the "softer" environment variables, like community building, prestige, or dissemination strategies. 

There is no need to replicate Language Science Press in the strict sense since one commmunity-run publisher per subfield should do. But the model, procedures, data, and tools could be replicated for other fields, from Archaeology to Zoology. Obviously, the context will be different, hence we cannot expect identical results, but if those other projects also release their data, we will have a growing pool of empirical information about how to further publication models which do not stand in the way of replicability as do the legacy models: these will be replicable publication models to ensure replicability in science.